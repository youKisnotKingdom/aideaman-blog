---
title: 'Open WebUI + ollama + Fugaku-LLM with Docker'
description: 'Lorem ipsum dolor sit amet'
pubDate: '2024/05/13'
heroImage: '/blog-placeholder-3.jpg'
---

Open WebUIとollamaを使って、"Fugaku-LLM/Fugaku-LLM-13B"のggufモデルをChatGPTライクに実行できる環境を作成します。  
サービスは`docker compose`を用いて起動します。  
すでにいくつか記事も存在していますが、ここでは`docker compose`を用いたセットアップについて記します。


#### サーバーの環境

```bash
Ubuntu 22.04.4 LTS
```
  

#### 環境設定

##### Step 1: Clone the Repository
まず、作業するディレクトリを作成します。今回は`fugaku`にしました。  
`fugaku`フォルダに移動して、Open WebUIのリポジトリを自分の環境にcloneします。

```bash:/fugaku
git clone https://github.com/open-webui/open-webui.git
cd open-webui
```
`docker compose up`を使うと、ollamaのコンテナも同時に立ち上げることができます。
NVIDIA GPUやAMD GPUを有効化するには、下のコマンドを実行してください。

```bash:/fugaku/open-webui
docker compose up -d --build

#NVIDIA GPUを有効化する場合
docker compose -f docker-compose.yaml -f docker-compose.gpu.yaml up -d --build

#AMD GPUを有効化する場合
HSA_OVERRIDE_GFX_VERSION=11.0.0 docker compose -f docker-compose.yaml -f docker-compose.amdgpu.yaml up -d --build
```
特にエラーなく動けばOK
`docker compose down`を実行して、一度サービスを落とします。

##### Step 2: モデルの準備
Hugging Face上にある[Fugakuのggufモデル](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf/tree/main)をPCにダウンロードし、`fugaku`配下にコピーします。
ディレクトリの構造は以下のようになります。
```bash
/path/to/fugaku
|--Fugaku-LLM-13B-instruct-0325b.gguf
|--open-webui
```

モデル準備用の仮の`docker-compose.yaml`を作成するために、open-webuiフォルダからコピーします。

```bash:/fugaku
 cp open-webui/docker-compose.yaml .
```

docker-compose.yamlを編集します。

```diff:docker-compose.yaml
version: '3.8'

services:
  ollama:
    volumes:
      - ollama:/root/.ollama
+     - ./:/fugaku             #<--この行を追加
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
```
また、モデルのロードに使うためのModelfileファイルを追加します。  
内容は[こちら](https://zenn.dev/hellorusk/articles/94bf32ea09ba26)から参照しました。ありがとうございます。  
1行目の.ggufファイルはダウンロードしたファイル名と同じものを設定してください。

```txt:Modelfile
FROM ./Fugaku-LLM-13B-instruct-0325b-q5_k_m.gguf


PARAMETER repeat_penalty 1.0
PARAMETER temperature 0.1
PARAMETER top_k 0
PARAMETER top_p 1.0
PARAMETER stop "<EOD|LLM-jp>"

TEMPLATE """{{ if .System }}{{ .System }}{{ end }}{{ if .Prompt }}

### 指示:
{{ .Prompt }}{{ end }}

### 応答:
{{ .Response }}<EOD|LLM-jp>"""
SYSTEM """以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。"""
```

ファイルが作成できたら、`fugaku`フォルダの配下にあることを確認し、`docker compose up`で立ち上げます。  
その後、`docker compose exec`でollamaの内部のbashに入ります。

```bash:/fugaku
 docker compose up -d
 docker compose exec ollama bash
```

```bash:ollama内のbash
cd fugaku/
ollama create fugaku-llm-13b-instruct -f ./Modelfile
```
少し時間はかかりますが、successと出ていれば無事完了です。

##### Step 3: 立ち上げ
最後にもう一度open-webuiディイレクトリに移動して、docker compose upで立ち上げます。
```bash:/fugaku
cd open-webui
docker-compose up -d
```
http://localhost:3000 に遷移しログイン後、画面左上の`Select a model`でFugakuモデルが選択できたらOK


#### Troubleshooting