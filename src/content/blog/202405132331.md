---
title: 'Open WebUI + ollama + Fugaku-LLM with Docker'
description: 'Lorem ipsum dolor sit amet'
pubDate: '2024/05/13'
heroImage: '/blog-placeholder-3.jpg'
---

Open WebUIとollamaを使って、"Fugaku-LLM/Fugaku-LLM-13B"のggufモデルをChatGPTライクに実行できる環境を作成します。
サービスはDockerを用いて起動します。

#### サーバーの環境

```bash
ubuntu 24.04
```

#### 環境設定

##### Step 1: Clone the Repository
まず、作業するディレクトリを作成します。今回は`fugaku`  
Open WebUIのリポジトリを自分の環境にcloneします。

```bash:/fugaku
git clone https://github.com/open-webui/open-webui.git
cd open-webui
```
ollamaをまだ入れていない場合は、docker composeを使って全部入れちゃいます。
NVIDIA GPUやAMD GPUを有効化するには、下のコマンドを実行してください。

```bash:/fugaku/open-webui
docker compose up -d --build

#NVIDIA GPUを有効化する場合
docker compose -f docker-compose.yaml -f docker-compose.gpu.yaml up -d --build

#AMD GPUを有効化する場合
HSA_OVERRIDE_GFX_VERSION=11.0.0 docker compose -f docker-compose.yaml -f docker-compose.amdgpu.yaml up -d --build
```
特にエラーなく動けばOK

##### Step 2: モデルの準備
Hugging Face上にあるFugakuモデルをPCにダウンロードし、`fugaku`配下にコピーします。
ディレクトリの構造は以下のようになります。
```bash
/path/to/fugaku
|--Fugaku-LLM-13B-instruct-0325b.gguf
|--open-webui
```
みたいになってればOK

モデルをollamaで使えるようにするために、仮の`docker-compose.yaml`をopen-webuiフォルダからコピーします。
```bash:/fugaku/open-webui
 cp open-webui/docker-compose.yaml .
```
docker-compose.yamlを編集します。
```diff:docker-compose.yaml
version: '3.8'

services:
  ollama:
    volumes:
      - ollama:/root/.ollama
+     - ./:/fugaku             #<--この行を追加
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
```
また、モデルのロードに使うためのModelfileファイルを追加します。  
内容はこちらから参照しました。ありがとうございます。  
1行目の.ggufファイルは自分の環境と同じものを設定してください。

```Modelfile
FROM ./Fugaku-LLM-13B-instruct-0325b-q5_k_m.gguf


PARAMETER repeat_penalty 1.0
PARAMETER temperature 0.1
PARAMETER top_k 0
PARAMETER top_p 1.0
PARAMETER stop "<EOD|LLM-jp>"

TEMPLATE """{{ if .System }}{{ .System }}{{ end }}{{ if .Prompt }}

### 指示:
{{ .Prompt }}{{ end }}

### 応答:
{{ .Response }}<EOD|LLM-jp>"""
SYSTEM """以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。"""
```
`docker compose up`で立ち上げて、`docker compose exec`でollamaの内部のbashに入ります。

```bash:/fugaku
 docker compose up -d
 docker compose exec ollama bash
```

```bash:ollama内のbash
cd fugaku/
ollama create fugaku-llm-13b-instruct -f ./Modelfile
```
successと出ていれば無事完了です。

##### Step 3: 立ち上げ
最後にもう一度open-webuiディイレクトリに移動して、docker compose upで立ち上げます。
```bash:/fugaku/open-webui
cd open-webui
docker-compose up -d
```
http://localhost:3000 を表示して、select modelでfugakuモデルが出てきたらOK


#### ollamaをすでに入れている場合

ollamaをホストの環境に直接インストールしている場合、以下の手順に従ってFugakuモデルを使用する準備をします。

##### ollamaが直接インストールされている場合
ホスト環境にollamaが直接インストールされている場合は、次のコマンドを使用してFugakuモデルのggufファイルをollamaで使用可能な形式に変換します。

```bash
ollama create fugaku-llm-13b-instruct -f /path/to/your/Modelfile
```

`/path/to/your/Modelfile`は、ホストシステムに保存されている`Modelfile`のパスを指しています。このファイルには、モデルをロードするための指示が含まれています。

##### Dockerを使用してollamaを起動している場合
ollamaがDockerコンテナ内で動作している場合は、以下のコマンドを使用してollamaのコンテナに接続し、必要な設定を行います。

```bash:/fugaku
docker run -it -v ./ ollama/ollama bash
```

このコマンドは、ホストのモデルディレクトリをコンテナの`/fugaku`ディレクトリにマウントします。次に、コンテナ内でコマンドを実行してモデルを変換します。

```bash
cd /fugaku
ollama create fugaku-llm-13b-instruct -f ./Modelfile
```

##### Step 3: docker-compose.yamlの編集
ollama環境が準備できた後、`docker-compose.yaml`を編集します。
別にollamaを立ち上げている場合は、open-webuiの環境変数を修正必要があります。

```diff:docker-compose.yaml
...

 open-webui:
    build:
      context: .
      args:
        OLLAMA_BASE_URL: '/ollama'
      dockerfile: Dockerfile
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
-      - 'OLLAMA_BASE_URL=http://ollama:11434' 
+      - 'OLLAMA_BASE_URL=http://host.docker.internal:11434'
      - 'WEBUI_SECRET_KEY='
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

```

最後に、`docker-compose up -d`を実行して環境を起動し、http://localhost:3000 にアクセスしてモデルが正しくロードされているか確認してください。

#### Troubleshooting